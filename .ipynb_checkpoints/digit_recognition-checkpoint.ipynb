{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707dff8d-f7b2-4217-a0e8-08bbe2d8c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad4027f-762c-4e07-9e06-2967954f445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\" Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def mse(y_pred, y):\n",
    "    \"\"\"Mean Squared Error (MSE) cost function.\"\"\"\n",
    "    return np.mean((y_pred - y) ** 2)\n",
    "\n",
    "def mse_prime(y_pred, y):\n",
    "    \"\"\"Derivative of the Mean Squared Error (MSE) cost function.\"\"\"\n",
    "    return 2 * (y_pred - y) / y.shape[1]\n",
    "    \n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "def softmax_prime(z):\n",
    "    s = softmax(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f8010935-e015-495d-bbcc-aaf53268c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with random weights and biases.\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes (list): List of integers specifying the number of neurons in each layer.\n",
    "                               Example: [input_size, hidden_size, ..., output_size]\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "        self.biases = [np.random.randn(y, 1) for y in layer_sizes[1:]]\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        # Reshape input to a column vector\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        \n",
    "        self.a = [X] # List to store activations for each layer\n",
    "        self.z = [] # List to store weighted inputs for each layer\n",
    "\n",
    "        for l, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            z = w @ self.a[-1] + b # Weighted input\n",
    "            self.z.append(z) \n",
    "\n",
    "            # Use softmax for last layer sigmoid for hidden layers\n",
    "            if l == len(self.biases) - 1: # Last layer (output)\n",
    "                a = softmax(z) # Softmax for output\n",
    "            else:\n",
    "                a = sigmoid(z) # Sigmoid for hidden\n",
    "            \n",
    "            self.a.append(a) # Activation\n",
    "\n",
    "        return self.a[-1] # Output of the network\n",
    "    \n",
    "\n",
    "    def backprop(self, y):\n",
    "        \"\"\"\n",
    "        Compute gradients using backpropagation.\n",
    "        \n",
    "        Args:\n",
    "            y: Target outputs of shape (output_size, batch_size).\n",
    "        \n",
    "        Returns:\n",
    "            Tuple (grad_w, grad_b) containing gradients for each layer.\n",
    "        \"\"\"\n",
    "        # Initialize gradient lists\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Output layer error (delta_L)\n",
    "        delta = mse_prime(self.a[-1], y) * softmax_prime(self.z[-1])\n",
    "        # Store output layer gradients\n",
    "        grad_w[-1] = delta @ self.a[-2].T\n",
    "        grad_b[-1] = delta\n",
    "        \n",
    "        # Propagate error backward through hidden layers\n",
    "        for l in reversed(range(len(self.weights) - 1)):\n",
    "            # Calculate delta for current layer\n",
    "            delta = (self.weights[l + 1].T @ delta) * sigmoid_prime(self.z[l])\n",
    "            # Compute gradients\n",
    "            grad_w[l] = delta @ self.a[l].T\n",
    "            grad_b[l] = delta\n",
    "        \n",
    "        return grad_w, grad_b\n",
    "        \n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, eta, batch_size):\n",
    "        \"\"\"Update weights and biases using averaged gradients.\"\"\"\n",
    "        for l in range(len(self.weights)):\n",
    "            self.weights[l] -= (eta / batch_size) * grad_w[l]\n",
    "            self.biases[l] -= (eta / batch_size) * grad_b[l]\n",
    "            \n",
    "\n",
    "    def train(self, x_train, y_train, epochs, batch_size, eta, results=False):\n",
    "        \"\"\"\n",
    "        Train the network using mini-batch SGD.\n",
    "        \n",
    "        Args:\n",
    "            x_train: Input data of shape (num_samples, input_size).\n",
    "            y_train: Target outputs of shape (num_samples, output_size).\n",
    "            epochs: Number of training epochs.\n",
    "            batch_size: Size of mini-batches.\n",
    "            eta: Learning rate.\n",
    "        \"\"\"\n",
    "        num_samples = len(x_train)\n",
    "        input_size = self.layer_sizes[0]\n",
    "        output_size = self.layer_sizes[-1]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(num_samples)\n",
    "            X_shuffled = [x_train[i] for i in indices]\n",
    "            y_shuffled = [y_train[i] for i in indices]\n",
    "\n",
    "            if results:\n",
    "                # Initialize lists to store outputs and true labels\n",
    "                epoch_outputs = []\n",
    "                epoch_labels = []\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                # Get mini-batch\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Initialize gradient arrays\n",
    "                grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "                grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "                \n",
    "                # Accumulate gradients over mini-batch\n",
    "                for x, y_true in zip(X_batch, y_batch):\n",
    "                    # Convert to column vectors\n",
    "                    x = x.reshape(-1, 1)\n",
    "                    y_true = y_true.reshape(-1, 1)\n",
    "\n",
    "                    \n",
    "                    # Forward + Backprop\n",
    "                    output = self.forward(x)\n",
    "                    batch_grad_w, batch_grad_b = self.backprop(y_true)\n",
    "\n",
    "                    if results:\n",
    "                        epoch_outputs.append(output.flatten())  # Save output\n",
    "                        epoch_labels.append(y_true.flatten())  # Save true label\n",
    "                    \n",
    "                    # Add to total gradients\n",
    "                    for l in range(len(self.weights)):\n",
    "                        grad_w[l] += batch_grad_w[l]\n",
    "                        grad_b[l] += batch_grad_b[l]\n",
    "                \n",
    "                # Update parameters (gradients are averaged here)\n",
    "                self.update_parameters(grad_w, grad_b, eta, batch_size)\n",
    "\n",
    "            \n",
    "            # Optional: Print progress\n",
    "            if results && (epoch % 10 == 0): \n",
    "                # Convert lists to numpy arrays\n",
    "                epoch_outputs = np.array(epoch_outputs)\n",
    "                epoch_labels = np.array(epoch_labels)\n",
    "    \n",
    "                # Calculate loss (MSE)\n",
    "                loss = mse(epoch_outputs, epoch_labels)\n",
    "    \n",
    "                # Calculate accuracy\n",
    "                predictions = np.argmax(epoch_outputs, axis=1)\n",
    "                true_labels = np.argmax(epoch_labels, axis=1)\n",
    "                accuracy = np.mean(predictions == true_labels)\n",
    "    \n",
    "                # Print progress\n",
    "                print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bc624376-83d0-42b2-b2de-ae421fef2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data to the range [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Flatten the images (28x28 -> 784)\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6a456832-b1d1-4af2-ae16-66aa998a2128",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,1) (60000,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      6\u001b[0m eta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[159], line 132\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, x_train, y_train, epochs, batch_size, eta, results)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results: \n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Optional: Print progress\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mmse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mmse\u001b[0;34m(y_pred, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmse\u001b[39m(y_pred, y):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean Squared Error (MSE) cost function.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean((\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,1) (60000,10) "
     ]
    }
   ],
   "source": [
    "layer_sizes = [784, 16, 16, 10]\n",
    "nn = NeuralNetwork(layer_sizes)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "eta = 0.01\n",
    "\n",
    "nn.train(x_train, y_train, epochs, batch_size, eta, results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49890675-05fe-4a9c-9baa-df217ee3d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "epoch loop\n",
    "    shuffle(training_data)\n",
    "    break data into minibatches according to batch size\n",
    "    loop over minibatches\n",
    "        for a single minibatch loop over the training data inside it\n",
    "            forward pass with training example as input\n",
    "            calculate gradient using backprop\n",
    "        average over the components inside the gradients respectfully to compute an estimate of the total gradient of the cost function \n",
    "        update weights and biases according to the learning rate and the gradient (estimated over the gradients of each training example in a minibatch)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
